{"chunks": [{"type": "docx", "chunk_number": 1, "paragraphs": [{"paragraph_number": 1, "text": "Simple Website Analytics Dashboard"}, {"paragraph_number": 2, "text": ""}, {"paragraph_number": 3, "text": "Problem Statement:\u00a0A small business wants to track website traffic and user engagement to understand which pages are most popular and how users are navigating their site."}, {"paragraph_number": 4, "text": "Objective: "}, {"paragraph_number": 5, "text": "Monitor website traffic pattern "}, {"paragraph_number": 6, "text": "Identify most visited pages"}, {"paragraph_number": 7, "text": "Understand user navigation path"}, {"paragraph_number": 8, "text": "Solution Overview:\u00a0Build a basic analytics pipeline in Microsoft Fabric to ingest website log data, transform it, and visualize key metrics in a Power BI dashboard."}, {"paragraph_number": 9, "text": "Fabric Components: "}, {"paragraph_number": 10, "text": "Data Factory: Dataflow Gen2 for Data Ingestion and Transformation with Data Pipeline for Orchestration."}, {"paragraph_number": 11, "text": "Data Engineering: Lakehouse for unified data storage."}, {"paragraph_number": 12, "text": "Power BI: Report for Data Visualization and Reporting and a Dashboard for visual summary"}, {"paragraph_number": 13, "text": "Implementation Steps:"}, {"paragraph_number": 14, "text": "Create a dataflow from source (on-premises csv file) and destination as bronze layer of Lakehouse."}, {"paragraph_number": 15, "text": "Create a data pipeline with the dataflow made previously and perform basic data cleaning step and the destination of our pipeline to silver layer of our Lakehouse."}, {"paragraph_number": 16, "text": "Create a new Dataflow or use existing one for data enrichment for analytics-ready data."}, {"paragraph_number": 17, "text": "Create a Report for data visualization and dashboard generation."}, {"paragraph_number": 18, "text": "Architecture: "}, {"paragraph_number": 19, "text": "Use Cases:"}, {"paragraph_number": 20, "text": "Website traffic: Track the total number of user visits per day to monitor overall site engagement trends"}, {"paragraph_number": 21, "text": "Specific page traffic: Analyse user visits per individual page to identify content popularity and performance."}, {"paragraph_number": 22, "text": "Navigation path: Map the most common user navigation flows to understand user behaviour and optimize site structure."}, {"paragraph_number": 23, "text": "Average session time of users per page: Measure how long users stay on each page to assess content engagement and effectiveness."}, {"paragraph_number": 24, "text": "High traffic hours on the website: Identify peak hours of user activity to optimize content delivery and server performance."}, {"paragraph_number": 25, "text": ""}, {"paragraph_number": 26, "text": "WORKFLOW"}, {"paragraph_number": 27, "text": ""}, {"paragraph_number": 28, "text": "Practical Workflow:"}, {"paragraph_number": 29, "text": "Data Curation: "}, {"paragraph_number": 30, "text": "Kaggle datasets: "}, {"paragraph_number": 31, "text": "https://www.kaggle.com/datasets/eliasdabbas/web-server-access-logs?select=access.log"}, {"paragraph_number": 32, "text": "It contains two files:"}, {"paragraph_number": 33, "text": "Access.log (~3.5GB): It contains 1Cr+ Records"}, {"paragraph_number": 34, "text": "IP Address"}, {"paragraph_number": 35, "text": "Time Stamp"}, {"paragraph_number": 36, "text": "Http Method"}, {"paragraph_number": 37, "text": "URI/Page Path"}, {"paragraph_number": 38, "text": "Http Status"}, {"paragraph_number": 39, "text": "Bytes Sent"}, {"paragraph_number": 40, "text": "Referrer/Previous Page"}, {"paragraph_number": 41, "text": "User Agent/Browser Information"}, {"paragraph_number": 42, "text": "client_hostname.csv: It contains 2.5L+ Records"}, {"paragraph_number": 43, "text": "Client IP Address"}, {"paragraph_number": 44, "text": "Hostname"}, {"paragraph_number": 45, "text": "Alias List"}, {"paragraph_number": 46, "text": "Address List"}, {"paragraph_number": 47, "text": "The client_hostname.csv will not be used by us since it does not have any useful data."}, {"paragraph_number": 48, "text": "We will be dividing Access.log file into two files:"}, {"paragraph_number": 49, "text": "Session.csv:"}, {"paragraph_number": 50, "text": "We will first convert the log file into csv through a python script."}, {"paragraph_number": 51, "text": "Using the same access.csv we will first reduce the records by:"}, {"paragraph_number": 52, "text": "Create a page depth column and filter the rows on depth less than 4"}, {"paragraph_number": 53, "text": "Then we filter the Url column based on does not contain -, %, ?, (,)"}, {"paragraph_number": 54, "text": "Finally remove the page depth column and our session.csv fact table will be ready"}, {"paragraph_number": 55, "text": ""}, {"paragraph_number": 56, "text": "IP Address"}, {"paragraph_number": 57, "text": "Time Stamp"}, {"paragraph_number": 58, "text": "Http Method"}, {"paragraph_number": 59, "text": "URI/Page Path"}, {"paragraph_number": 60, "text": "Http Status"}, {"paragraph_number": 61, "text": "Bytes Sent"}, {"paragraph_number": 62, "text": "Referrer/Previous Page"}, {"paragraph_number": 63, "text": "User Agent/Browser Information"}, {"paragraph_number": 64, "text": ""}, {"paragraph_number": 65, "text": "Page_table.csv"}, {"paragraph_number": 66, "text": "We will create a copy of session.csv and filter the Uri column on delete duplicate "}, {"paragraph_number": 67, "text": "Then we will create a page depth column and remove all the depth greater than 3"}, {"paragraph_number": 68, "text": ""}, {"paragraph_number": 69, "text": "Page_url"}, {"paragraph_number": 70, "text": "Page_type"}, {"paragraph_number": 71, "text": "Page_depth: "}, {"paragraph_number": 72, "text": "We have our data ready now"}, {"paragraph_number": 73, "text": ""}, {"paragraph_number": 74, "text": "Data Architecture:"}, {"paragraph_number": 75, "text": "Configure what feature to use for each step in the architecture"}, {"paragraph_number": 76, "text": ""}, {"paragraph_number": 77, "text": "For storage we will use lake house for bronze and silver layer and a warehouse as our gold layer."}, {"paragraph_number": 78, "text": ""}, {"paragraph_number": 79, "text": "For Initial data ingestion into the bronze layer, we will directly upload data into the Files of our Lakehouse since we have static data."}, {"paragraph_number": 80, "text": ""}, {"paragraph_number": 81, "text": "For Data loading into the silver layer, we will use Dataflow Gen2 since, it can also be used for the initial cleaning of the data and the loading part as well. It also comes with benefits like "}, {"paragraph_number": 82, "text": "Flexible schema operations, "}, {"paragraph_number": 83, "text": "Automatic data quality assessment and "}, {"paragraph_number": 84, "text": "Pattern detection."}, {"paragraph_number": 85, "text": ""}, {"paragraph_number": 86, "text": "For Data loading into the gold layer, we will again use Dataflow Gen2 since again providing the benefits earlier mentioned and it give us "}, {"paragraph_number": 87, "text": "Low-code or no-code transformation of data."}, {"paragraph_number": 88, "text": "Supports Incremental refresh (reduces load time)."}, {"paragraph_number": 89, "text": "Seamless integration with data stores"}, {"paragraph_number": 90, "text": "Whereas Notebook and SQL scripting requires Coding and has less UI support making Dataflow a better choice given our data source is not large."}, {"paragraph_number": 91, "text": "We can also use Data Pipeline for automating or orchestrating purpose."}, {"paragraph_number": 92, "text": ""}, {"paragraph_number": 93, "text": "Fabric Implementation:"}, {"paragraph_number": 94, "text": ""}, {"paragraph_number": 95, "text": "Create a Workspace (project_usecase)"}, {"paragraph_number": 96, "text": "Create a Lakehouse for Broze and Silver layer. (MyLakehouse)"}, {"paragraph_number": 97, "text": "Create a Warehouse for Gold layer. (MyWarehouse)"}, {"paragraph_number": 98, "text": ""}, {"paragraph_number": 99, "text": "Create a subfolder (Bronze) in the File folder of MyLakehouse and upload the csv files."}, {"paragraph_number": 100, "text": ""}, {"paragraph_number": 101, "text": "Create a DataFlow Gen2 (DataBronzetoSilver) with source as our csv files and destination as My_Lakehouse and rename the tables as: Silver_PageTable and Silver_Session."}, {"paragraph_number": 102, "text": ""}, {"paragraph_number": 103, "text": "Perform basic Data Cleaning and Transformations using DataFlow."}, {"paragraph_number": 104, "text": ""}, {"paragraph_number": 105, "text": "Page_table.csv:"}, {"paragraph_number": 106, "text": "Promote header"}, {"paragraph_number": 107, "text": "Correct datatype"}, {"paragraph_number": 108, "text": "Filter rows to reduce the number of rows like: keep rows_depth<4 and remove url containing charcters like %, - ."}, {"paragraph_number": 109, "text": "Remove duplicates"}, {"paragraph_number": 110, "text": "Remove errors"}, {"paragraph_number": 111, "text": "Session.csv:"}, {"paragraph_number": 112, "text": "Promote header"}, {"paragraph_number": 113, "text": "Split column timestamp to split timezone."}, {"paragraph_number": 114, "text": "Remove timezone column"}, {"paragraph_number": 115, "text": "Correct datatypes"}, {"paragraph_number": 116, "text": "Remove columns like method, protocol,status,size since of no use."}, {"paragraph_number": 117, "text": "Split column timestamp to date and time."}, {"paragraph_number": 118, "text": "Rename all column to more sensible name."}, {"paragraph_number": 119, "text": "Remove duplicate rows."}, {"paragraph_number": 120, "text": "Remove errors."}, {"paragraph_number": 121, "text": ""}, {"paragraph_number": 122, "text": "Create a Data Pipeline (Data Cleaning) and add the dataflow (DataBronzetoSilver) to it."}, {"paragraph_number": 123, "text": "Similarly Create Five Different Dataflow for each use case to apply business logic to the silver layer cleaned data with source as the silver layer table in MyLakehouse and destination as OurWarehouse."}, {"paragraph_number": 124, "text": "Apply the following Business logic/Transformations:"}, {"paragraph_number": 125, "text": ""}, {"paragraph_number": 126, "text": "Visit per day"}, {"paragraph_number": 127, "text": "Get data session table"}, {"paragraph_number": 128, "text": "Remove the unwanted columns like time, referrer, user_agent, url"}, {"paragraph_number": 129, "text": "Group data into date on count rows."}, {"paragraph_number": 130, "text": "We get the count of visits per day."}, {"paragraph_number": 131, "text": "PerPageTraffic"}, {"paragraph_number": 132, "text": "Get data session table and page table"}, {"paragraph_number": 133, "text": "Merge the table and perform inner join on url "}, {"paragraph_number": 134, "text": "Choose columns page_url, page_type, page_depth"}, {"paragraph_number": 135, "text": "Group data into url on count rows."}, {"paragraph_number": 136, "text": "Hourly Traffic:"}, {"paragraph_number": 137, "text": "Get data session table."}, {"paragraph_number": 138, "text": "Group data into time on count rows."}, {"paragraph_number": 139, "text": "Sort the count column to ascending order."}, {"paragraph_number": 140, "text": "Rename columns"}, {"paragraph_number": 141, "text": "Navigation"}, {"paragraph_number": 142, "text": "Remove unwanted columns."}, {"paragraph_number": 143, "text": "Split the referrer column to get only the url path and not the website."}, {"paragraph_number": 144, "text": "Delete the website column."}, {"paragraph_number": 145, "text": "Rename the columns"}, {"paragraph_number": 146, "text": "Remove duplicates"}, {"paragraph_number": 147, "text": "Remove errors"}, {"paragraph_number": 148, "text": "Create a custom column with from_page -> to_page"}, {"paragraph_number": 149, "text": "Group data into the navigation path on count rows"}, {"paragraph_number": 150, "text": "Duplicate the navigation path column"}, {"paragraph_number": 151, "text": "Split the duplicate to get from_page and to_page"}, {"paragraph_number": 152, "text": "Average time per page"}, {"paragraph_number": 153, "text": "Create a custom column with both date and time"}, {"paragraph_number": 154, "text": "Remove date and time column."}, {"paragraph_number": 155, "text": "Sort the data into ascending order of date."}, {"paragraph_number": 156, "text": "Create a duplicate of the table (PreviousRows)"}, {"paragraph_number": 157, "text": "Insert index into the initial table starting with 0"}, {"paragraph_number": 158, "text": "Insert index into the duplicate of the table starting with 1."}, {"paragraph_number": 159, "text": "Merge the tables with the index with inner join"}, {"paragraph_number": 160, "text": "Expand to get the datetime."}, {"paragraph_number": 161, "text": "Rename the columns."}, {"paragraph_number": 162, "text": "Add custom columns calculating the time difference."}, {"paragraph_number": 163, "text": "Group data into url on average time difference."}, {"paragraph_number": 164, "text": "Sort in descending order of avg time."}, {"paragraph_number": 165, "text": ""}, {"paragraph_number": 166, "text": "Create a Data Pipeline (DataEnrichment) and insert all the Dataflows into it."}, {"paragraph_number": 167, "text": "Now we have Enriched data ready for creating insights and report/dashboard."}, {"paragraph_number": 168, "text": "Now to create a report first we need to make a semantic model (Report Model) from the warehouse with all the gold layer tables. "}, {"paragraph_number": 169, "text": "Create a blank report with the semantic model."}, {"paragraph_number": 170, "text": "Insert a text box for heading \u201cWebsite User-Behavior Report\u201d."}, {"paragraph_number": 171, "text": "From data panel select count and page_url from Gold_Perpagetraffic table."}, {"paragraph_number": 172, "text": "From visualization pane select \u2018pie chart\u2019 "}, {"paragraph_number": 173, "text": "Configure the colour based on your view for the dashboard."}, {"paragraph_number": 174, "text": "From format visuals in Visualization pane apply necessary ui changes."}, {"paragraph_number": 175, "text": ""}, {"paragraph_number": 176, "text": "From data panel select count and date from Gold_VisitsPerDay table."}, {"paragraph_number": 177, "text": "Ensure \u2018Stacked column chart\u2019 is selected in visualization pane and x-axis had date and y axis has sum of counts"}, {"paragraph_number": 178, "text": "From format visuals in Visualization pane apply necessary ui changes."}, {"paragraph_number": 179, "text": ""}, {"paragraph_number": 180, "text": "From Data pane select count and time from Gold_HourlyTraffic table."}, {"paragraph_number": 181, "text": "Select line chart from visualization and format necessary visuals."}, {"paragraph_number": 182, "text": "Ensure time is in x axis and count in y-axis."}, {"paragraph_number": 183, "text": "From Data pane select count and url from Gold_AverageTimePerPage."}, {"paragraph_number": 184, "text": "Select stacked column chart for visualisation."}, {"paragraph_number": 185, "text": "Ensure x axis as url and y axis as count and ensure necessary colour label and other changes."}, {"paragraph_number": 186, "text": "From data pane select count, from_table, to_table from Navigation table."}, {"paragraph_number": 187, "text": "In visualization pane select \u2026 and click \u201cget more visuals\u201d and search for Sankey diagram and use it."}, {"paragraph_number": 188, "text": "Configure necessary colour and other ui changes to make it look more attractive."}, {"paragraph_number": 189, "text": ""}, {"paragraph_number": 190, "text": "Takeaways from the report:"}, {"paragraph_number": 191, "text": "In the bar chart of Average time per page we get the webpages with most user interaction so we can understand the likings of the target customers to understand customer interests."}, {"paragraph_number": 192, "text": "In the pie chart of Traffic per page we can see which pages are most visited. We can also compare it with the avg time bar chart to understand which pages have a high traffic but users donot stay for long."}, {"paragraph_number": 193, "text": "In the hourly traffic line chart we can see at which time in the day does our customers are more active. "}, {"paragraph_number": 194, "text": "In the Visits by date bar chart we can understand during which days of the week do customers visit the site most frequently."}, {"paragraph_number": 195, "text": "In the Sankey diagram of navigation path we can understand the volatile interest of customers and how the current suggestions work on the website."}, {"paragraph_number": 196, "text": ""}, {"paragraph_number": 197, "text": "Further improvements: "}, {"paragraph_number": 198, "text": ""}, {"paragraph_number": 199, "text": "For the current scenario we have used services that fit to what we currently have with us. To further improve our study, we will need to find a better and accurate data from a reliable source."}, {"paragraph_number": 200, "text": "Currently we have a static data resource directly uploaded into the bronze layer. In case we get a data that is constantly updating and new data in constantly getting added. We will have to orchestrate that using pipeline so that in schedule interval new data in upserted into the project."}, {"paragraph_number": 201, "text": "In the current implementation we have gone for a more Drag and Drop and user-friendly way to solve our problem statement. Another approach to also apply complex logic into our report could be using notebook."}, {"paragraph_number": 202, "text": "Our Report currently have very basic charts with just two or three variables. We can further increase the variables to understand the user behaviour more."}, {"paragraph_number": 203, "text": "Our current Data transformation step had some flaws which we coved during our Data Enrichment stage so to make the work division balanced between the stage and effectively utilise the architecture we can have more balanced work division."}, {"paragraph_number": 204, "text": ""}, {"paragraph_number": 205, "text": "PHASE 2"}, {"paragraph_number": 206, "text": "We\u2019ll now work on a\u00a0multi-tenant website analytics dashboard\u00a0with the following features:"}, {"paragraph_number": 207, "text": "Dynamic ingestion of website logs with\u00a0tenant specific (simulate data accordingly)"}, {"paragraph_number": 208, "text": "Schema and row count validation"}, {"paragraph_number": 209, "text": "Centralized logging per tenant and step\u00a0"}, {"paragraph_number": 210, "text": "Power BI dashboard showing traffic trends, navigation paths, session times, and peak hours according to tenant."}, {"paragraph_number": 211, "text": ""}, {"paragraph_number": 212, "text": "Data Curation:"}, {"paragraph_number": 213, "text": "Since multi-tenant website log data is hard to find. I have generated dataset from co-pilot with similar table structure containing the following columns/fields: "}, {"paragraph_number": 214, "text": ""}, {"paragraph_number": 215, "text": "Session ID"}, {"paragraph_number": 216, "text": "Tenant"}, {"paragraph_number": 217, "text": "IP Address"}, {"paragraph_number": 218, "text": "Time Stamp"}, {"paragraph_number": 219, "text": "Http Method"}, {"paragraph_number": 220, "text": "URI/Page Path"}, {"paragraph_number": 221, "text": "Http Status"}, {"paragraph_number": 222, "text": "Bytes Sent"}, {"paragraph_number": 223, "text": "Referrer/Previous Page"}, {"paragraph_number": 224, "text": "User Agent/Browser Information"}, {"paragraph_number": 225, "text": ""}, {"paragraph_number": 226, "text": "The datasets are different and separate for both tenants. We have the dataset of both the tenants as a csv file. Using a python script I have inserted it in my on-premises MySQL database."}, {"paragraph_number": 227, "text": "Table Schema:"}, {"paragraph_number": 228, "text": "CREATE TABLE tenantb ("}, {"paragraph_number": 229, "text": "  `Session ID` VARCHAR(255),"}, {"paragraph_number": 230, "text": "  `Tenant` VARCHAR(255),"}, {"paragraph_number": 231, "text": "  `IP Address` VARCHAR(45),"}, {"paragraph_number": 232, "text": "  `Time Stamp` DATETIME,"}, {"paragraph_number": 233, "text": "  `Http Method` VARCHAR(10),"}, {"paragraph_number": 234, "text": "  `URI/Page Path` TEXT,"}, {"paragraph_number": 235, "text": "  `Http Status` INT,"}, {"paragraph_number": 236, "text": "  `Bytes Sent` INT,"}, {"paragraph_number": 237, "text": "  `Referrer/Previous Page` TEXT,"}, {"paragraph_number": 238, "text": "  `User Agent/Browser Information` TEXT"}, {"paragraph_number": 239, "text": ");"}, {"paragraph_number": 240, "text": ""}, {"paragraph_number": 241, "text": "Data Architecture: We will be using a similar kind of data storage architecture for the second phase as well."}, {"paragraph_number": 242, "text": "We will have a few changes on the Data Pipeline Activities that we use."}, {"paragraph_number": 243, "text": ""}, {"paragraph_number": 244, "text": "Fabric Implementation:"}, {"paragraph_number": 245, "text": "Create a Workspace (project_secondphase)"}, {"paragraph_number": 246, "text": "Create a Lakehouse for Broze and Silver layer. (MyLakehouse)"}, {"paragraph_number": 247, "text": "Create a Warehouse for Gold layer. (Warehouse)"}, {"paragraph_number": 248, "text": "Create a Data Pipeline (Data Ingestion) for the initial data loading from the source which in our case is MySQL database."}, {"paragraph_number": 249, "text": "Since we want an incremental loading for our usecase we will create a Registry/Watermark table in our warehouse.\nControl_Ingestion: table_name , last_run_timestamp"}, {"paragraph_number": 250, "text": "In our pipeline we will add a lookup activity (GetTableList) to get the list of tables."}, {"paragraph_number": 251, "text": "SELECT table_name FROM Control_Ingestion;"}, {"paragraph_number": 252, "text": "We will connect the LookUp to a ForEach activity [@activity('GetTableList').output.value]"}, {"paragraph_number": 253, "text": "So our forEach activity will run for each of the table."}, {"paragraph_number": 254, "text": "Inside our for each we have another LookUp activity (GetTime)."}, {"paragraph_number": 255, "text": "SELECT last_run_timestamp FROM Control_Ingestion WHERE table_name = '@{item().table_name}'"}, {"paragraph_number": 256, "text": "Next for Column level Validation:"}, {"paragraph_number": 257, "text": "Connect it to a Copy Data Activity with source as the database Query it to get only the column names and destination as a schema table(table_name_schema)."}, {"paragraph_number": 258, "text": "@{trim(concat(item().table_name, '_schema'))}"}, {"paragraph_number": 259, "text": "We will use a Notebook activity to validate the schema from aur database that is stored as a table and the existing table in our lakehouse."}, {"paragraph_number": 260, "text": "We have two parameters: table_name (@item().table_name) and "}, {"paragraph_number": 261, "text": "Schematable_name (@{trim(concat(item().table_name, '_schema'))})"}, {"paragraph_number": 262, "text": "The Notebook (Data Validation) Script is: "}, {"paragraph_number": 263, "text": "table_name = \"tenanta\""}, {"paragraph_number": 264, "text": "schematable_name = \"tenanta_schema\""}, {"paragraph_number": 265, "text": "from notebookutils import mssparkutils"}, {"paragraph_number": 266, "text": "df = spark.table(table_name)"}, {"paragraph_number": 267, "text": "df2 = spark.table(schematable_name)"}, {"paragraph_number": 268, "text": "df_column_names = set(df.columns)"}, {"paragraph_number": 269, "text": "df2_column_names = set(row['column_name'] for row in df2.select(\"column_name\").collect())"}, {"paragraph_number": 270, "text": "# Compare and return result"}, {"paragraph_number": 271, "text": "match_result = df_column_names == df2_column_names"}, {"paragraph_number": 272, "text": "# Exit the notebook with the result as a string"}, {"paragraph_number": 273, "text": "mssparkutils.notebook.exit(str(match_result))"}, {"paragraph_number": 274, "text": ""}, {"paragraph_number": 275, "text": "We will connect it to an If Condition which will take the exit value as the condition."}, {"paragraph_number": 276, "text": "If the exit value is true (Schema matches):"}, {"paragraph_number": 277, "text": "Then we will use a copy data activity to incrementally load data using the Registry table in our warehouse and the getTime lookup we used earlier."}, {"paragraph_number": 278, "text": "Source: SELECT * FROM `@{item().table_name}`"}, {"paragraph_number": 279, "text": "WHERE Time_stamp > '@{activity('GetTime').output.value[0].last_run_timestamp}'"}, {"paragraph_number": 280, "text": "For the destination we will have @{item().table_name} with append action."}, {"paragraph_number": 281, "text": ""}, {"paragraph_number": 282, "text": "After copy data we will use a script (UpdateTime)  to update the time in the registry table. "}, {"paragraph_number": 283, "text": "UPDATE Control_Ingestion "}, {"paragraph_number": 284, "text": "SET last_run_timestamp = '@{utcNow()}'"}, {"paragraph_number": 285, "text": "WHERE table_name = '@{item().table_name}'"}, {"paragraph_number": 286, "text": "If the exit value is false (schema mismatch):"}, {"paragraph_number": 287, "text": "We will use an outlook activity to send a mail to notify about the schema mismatch."}, {"paragraph_number": 288, "text": "Our Initial Data Ingestion is now done."}, {"paragraph_number": 289, "text": ""}, {"paragraph_number": 290, "text": ""}, {"paragraph_number": 291, "text": "Next for Data cleaning we will use a Dataflow Gen2 to clean and remove the noise from our dataset."}, {"paragraph_number": 292, "text": "We will have two sources: tenanta and tenantb"}, {"paragraph_number": 293, "text": "Cleaning transformations:"}, {"paragraph_number": 294, "text": "Remove duplicates"}, {"paragraph_number": 295, "text": "Filter Httpstatus column row to have only 200"}, {"paragraph_number": 296, "text": "Remove noise from url and referrer where if it contains \u2018?v\u2019 and something then we remove it from the url or referrer"}, {"paragraph_number": 297, "text": "Table.AddColumn(#\"Added custom\", \"Custom\", each Text.BeforeDelimiter([Url], \"?\"))"}, {"paragraph_number": 298, "text": "Then we will select columns and have only those with use."}, {"paragraph_number": 299, "text": "Table.SelectColumns(#\"Added custom 1\", {\"Session_ID\", \"Tenant\", \"IP_Address\", \"Time_stamp\", \"UserAgent\", \"Referrer new\", \"Custom\"})"}, {"paragraph_number": 300, "text": "Column renaming: "}, {"paragraph_number": 301, "text": "Table.RenameColumns(#\"Choose columns\", {{\"Referrer new\", \"Referrer\"}, {\"Custom\", \"URI\"}})"}, {"paragraph_number": 302, "text": ""}, {"paragraph_number": 303, "text": "We will perform these steps in both the tables and then we will append them to get a single Silver_sessionTable."}, {"paragraph_number": 304, "text": "Create a Data Pipeline (Data Cleaning) and add the dataflow in the pipeline for scheduling and orchestration."}, {"paragraph_number": 305, "text": "Now we have our silver level data ready."}, {"paragraph_number": 306, "text": ""}, {"paragraph_number": 307, "text": "For Gold layer we will create a Data Pipeline (Data Enrichment):"}, {"paragraph_number": 308, "text": "Add a notebook (Data Enrich):\n"}, {"paragraph_number": 309, "text": "#1. We will read the silver layer data int a data frame and add custom column of date and hour"}, {"paragraph_number": 310, "text": "df = spark.read.table(\"Silver_SessionData\")"}, {"paragraph_number": 311, "text": "from pyspark.sql.functions import to_date, hour"}, {"paragraph_number": 312, "text": "df = df.withColumn(\"Date\", to_date(\"Time_stamp\"))"}, {"paragraph_number": 313, "text": "df = df.withColumn(\"Hour\", hour(\"Time_stamp\"))"}, {"paragraph_number": 314, "text": ""}, {"paragraph_number": 315, "text": "#2. We will use Data Wrangler for traffic_trends table:"}, {"paragraph_number": 316, "text": "# Code generated by Data Wrangler for PySpark DataFrame"}, {"paragraph_number": 317, "text": "from pyspark.sql import functions as F"}, {"paragraph_number": 318, "text": "def traffic_trends(df):"}, {"paragraph_number": 319, "text": "\u00a0 \u00a0 # Performed 1 aggregation grouped on columns: 'URI', 'Tenant'"}, {"paragraph_number": 320, "text": "\u00a0 \u00a0 df = df.groupBy('URI', 'Tenant').agg(F.count('URI').alias('URI_count'))"}, {"paragraph_number": 321, "text": "\u00a0 \u00a0 df = df.dropna()"}, {"paragraph_number": 322, "text": "\u00a0 \u00a0 df = df.sort(df['URI'].asc(), df['Tenant'].asc())"}, {"paragraph_number": 323, "text": "\u00a0 \u00a0 return df"}, {"paragraph_number": 324, "text": ""}, {"paragraph_number": 325, "text": "df_traffic_trends = traffic_trends(df)"}, {"paragraph_number": 326, "text": "df_traffic_trends.write.mode(\"overwrite\").saveAsTable(\"Gold_Traffic_trends\")"}, {"paragraph_number": 327, "text": ""}, {"paragraph_number": 328, "text": "#3. Navigation Path"}, {"paragraph_number": 329, "text": "# Code generated by Data Wrangler for PySpark DataFrame"}, {"paragraph_number": 330, "text": "from pyspark.sql import functions as F"}, {"paragraph_number": 331, "text": "def Navigation_path(df):"}, {"paragraph_number": 332, "text": "\u00a0 \u00a0 # Remove rows where Referrer is null, empty, or NaN"}, {"paragraph_number": 333, "text": "\u00a0 \u00a0 df = df.filter("}, {"paragraph_number": 334, "text": "\u00a0 \u00a0 \u00a0 \u00a0 (F.col('Referrer').isNotNull()) &"}, {"paragraph_number": 335, "text": "\u00a0 \u00a0 \u00a0 \u00a0 (F.col('Referrer') != '') &"}, {"paragraph_number": 336, "text": "\u00a0 \u00a0 \u00a0 \u00a0 (~F.isnan(F.col('Referrer')))"}, {"paragraph_number": 337, "text": "\u00a0 \u00a0 )"}, {"paragraph_number": 338, "text": "\u00a0 \u00a0 # Perform aggregation"}, {"paragraph_number": 339, "text": "\u00a0 \u00a0 df = df.groupBy('Tenant', 'URI', 'Referrer').agg(F.count('IP_Address').alias('IP_Address_count'))"}, {"paragraph_number": 340, "text": "\u00a0 \u00a0 # Sort the DataFrame"}, {"paragraph_number": 341, "text": "\u00a0 \u00a0 df = df.sort(df['Tenant'].asc(), df['URI'].asc(), df['Referrer'].asc())"}, {"paragraph_number": 342, "text": "\u00a0 \u00a0 return df"}, {"paragraph_number": 343, "text": "df_Nav_Path = Navigation_path(df)"}, {"paragraph_number": 344, "text": "df_Nav_Path.write.mode(\"overwrite\").saveAsTable(\"Gold_Navigation_Path\")"}, {"paragraph_number": 345, "text": ""}, {"paragraph_number": 346, "text": "#4. Session_time:"}, {"paragraph_number": 347, "text": "import pandas as pd"}, {"paragraph_number": 348, "text": "from pyspark.sql import functions as F"}, {"paragraph_number": 349, "text": "def Session_time(df):"}, {"paragraph_number": 350, "text": "\u00a0 \u00a0 # Sort by columns: 'Tenant', 'Session_ID', 'Time_stamp'"}, {"paragraph_number": 351, "text": "\u00a0 \u00a0 df = df.sort(df['Tenant'].asc(), df['Session_ID'].asc(), df['Time_stamp'].asc())"}, {"paragraph_number": 352, "text": "\u00a0 \u00a0 # Group by 'Tenant' and 'Session_ID' and get first and last timestamps"}, {"paragraph_number": 353, "text": "\u00a0 \u00a0 df = df.groupBy('Tenant', 'Session_ID').agg("}, {"paragraph_number": 354, "text": "\u00a0 \u00a0 \u00a0 \u00a0 F.last('Time_stamp').alias('Time_stamp_last'),"}, {"paragraph_number": 355, "text": "\u00a0 \u00a0 \u00a0 \u00a0 F.first('Time_stamp').alias('Time_stamp_first')"}, {"paragraph_number": 356, "text": "\u00a0 \u00a0 )"}, {"paragraph_number": 357, "text": "\u00a0 \u00a0 df = df.dropna()"}, {"paragraph_number": 358, "text": "\u00a0 \u00a0 pandas_df = df.toPandas()"}, {"paragraph_number": 359, "text": "\u00a0 \u00a0 pandas_df['Time_stamp_last'] = pd.to_datetime(pandas_df['Time_stamp_last'])"}, {"paragraph_number": 360, "text": "\u00a0 \u00a0 pandas_df['Time_stamp_first'] = pd.to_datetime(pandas_df['Time_stamp_first'])"}, {"paragraph_number": 361, "text": "\u00a0 \u00a0 pandas_df['Session_Time'] = (pandas_df['Time_stamp_last'] - pandas_df['Time_stamp_first']).dt.total_seconds("}, {"paragraph_number": 362, "text": "\u00a0 \u00a0 # Filter out sessions longer than 30 minutes (1800 seconds)"}, {"paragraph_number": 363, "text": "\u00a0 \u00a0 pandas_df = pandas_df[pandas_df['Session_Time'] <= 1800]"}, {"paragraph_number": 364, "text": "\u00a0 \u00a0 return pandas_df"}, {"paragraph_number": 365, "text": "df_Session_time = Session_time(df)"}, {"paragraph_number": 366, "text": "spark_df_Session_time = spark.createDataFrame(df_Session_time)"}, {"paragraph_number": 367, "text": "# Overwrite the existing table with updated schema and data"}, {"paragraph_number": 368, "text": "spark_df_Session_time.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(\"Gold_AvgSession_time\")"}, {"paragraph_number": 369, "text": ""}, {"paragraph_number": 370, "text": "#5.  Peak_hours\nfrom pyspark.sql import functions as F"}, {"paragraph_number": 371, "text": ""}, {"paragraph_number": 372, "text": "def Peak_hours(df):"}, {"paragraph_number": 373, "text": "\u00a0 \u00a0 df = df.dropna()"}, {"paragraph_number": 374, "text": "\u00a0 \u00a0 # Convert integer hour to time string and rename column"}, {"paragraph_number": 375, "text": "\u00a0 \u00a0 df = df.withColumn('Formatted_Hour', F.date_format(F.to_timestamp(df['Hour'].cast('string'), 'H'), 'h:mm a'))"}, {"paragraph_number": 376, "text": "\u00a0 \u00a0 # Drop original Hour column"}, {"paragraph_number": 377, "text": "\u00a0 \u00a0 df = df.drop('Hour')"}, {"paragraph_number": 378, "text": "\u00a0 \u00a0 # Rename formatted column to Hour"}, {"paragraph_number": 379, "text": "\u00a0 \u00a0 df = df.withColumnRenamed('Formatted_Hour', 'Hour')"}, {"paragraph_number": 380, "text": "\u00a0 \u00a0 # Sort by Hour and Tenant"}, {"paragraph_number": 381, "text": "\u00a0 \u00a0 df = df.sort(df['Hour'].asc(), df['Tenant'].asc())"}, {"paragraph_number": 382, "text": "\u00a0 \u00a0 return df"}, {"paragraph_number": 383, "text": ""}, {"paragraph_number": 384, "text": "df_Peak_hours = Peak_hours(df)"}, {"paragraph_number": 385, "text": "df_Peak_hours.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(\"Gold_Peak_hours\")"}, {"paragraph_number": 386, "text": ""}, {"paragraph_number": 387, "text": "Connect the Notebook to SetVariable and create a variable named Tables: [\"gold_traffic_trends\",\"gold_navigation_path\",\"gold_avgsession_time\",\"gold_peak_hours\"]"}, {"paragraph_number": 388, "text": "Hardcoded variable for gold layer table names stored in our lakehouse"}, {"paragraph_number": 389, "text": "Add a ForEach statement and use the variable to run it for each of the tables and use copy data activity to copy the gold tables to warehouse."}, {"paragraph_number": 390, "text": ""}, {"paragraph_number": 391, "text": "In the warehouse"}, {"paragraph_number": 392, "text": "Create a dimension table using SQL Query\nCREATE TABLE Gold.Tenant ("}, {"paragraph_number": 393, "text": "\u00a0 \u00a0 Tenant VARCHAR(255),"}, {"paragraph_number": 394, "text": "\u00a0 \u00a0 TenantName VARCHAR(255)"}, {"paragraph_number": 395, "text": ");"}, {"paragraph_number": 396, "text": "INSERT into Gold.Tenant VALUES ('TenantA','A'),('TenantB','B');"}, {"paragraph_number": 397, "text": ""}, {"paragraph_number": 398, "text": " create a semantic model with all the gold layer tables and create a report."}, {"paragraph_number": 399, "text": "Use Silcer for a Select tenant drop down."}, {"paragraph_number": 400, "text": "And separate charts for each of the tables."}, {"paragraph_number": 401, "text": "Create hierarchy in peak_hours for drill down implementations."}, {"paragraph_number": 402, "text": "Also create a slicer for date filter in peak hours"}, {"paragraph_number": 403, "text": "In the navigation_path:"}, {"paragraph_number": 404, "text": "X axis: Referrer and URI (automatically creates hierarchy and drill down implemented)"}, {"paragraph_number": 405, "text": "Y axis: IP address count"}, {"paragraph_number": 406, "text": "Or "}, {"paragraph_number": 407, "text": "X axis: referrer"}, {"paragraph_number": 408, "text": "Y axis: IP address count"}, {"paragraph_number": 409, "text": "Legend: URI "}, {"paragraph_number": 410, "text": "For clustered column chart"}, {"paragraph_number": 411, "text": ""}, {"paragraph_number": 412, "text": ""}, {"paragraph_number": 413, "text": "Final report"}, {"paragraph_number": 414, "text": ""}, {"paragraph_number": 415, "text": ""}, {"paragraph_number": 416, "text": ""}, {"paragraph_number": 417, "text": ""}, {"paragraph_number": 418, "text": ""}, {"paragraph_number": 419, "text": ""}, {"paragraph_number": 420, "text": ""}], "token_count": 2092}]}